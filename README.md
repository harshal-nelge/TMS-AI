# TMS AI Document Processing System

**AI-powered logistics document processing and question answering system for Transportation Management Systems**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0-green.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31.0-red.svg)](https://streamlit.io)

## ğŸ¯ Overview

TMS AI is a POC system that enables users to upload logistics documents (PDFs, DOCX, TXT) and interact with them using natural language questions. The system uses Retrieval-Augmented Generation (RAG) to provide grounded answers with confidence scores, applies guardrails to prevent hallucinations, and can extract structured shipment data.

### Key Features

- ğŸ“„ **Document Upload & Processing**: Support for PDF, DOCX, and TXT logistics documents
- ğŸ’¬ **Natural Language Q&A**: Ask questions and get context-grounded answers with source citations
- ğŸ“Š **Structured Data Extraction**: Extract shipment information into JSON format
- ğŸ›¡ï¸ **Guardrails**: Confidence thresholds prevent low-quality answers
- ğŸ”„ **Retry Mechanism**: Exponential backoff for API rate limits
- ğŸ¨ **Modern UI**: Clean Streamlit interface for easy interaction

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streamlit UI Layer                       â”‚
â”‚          (Upload â”‚ Q&A Interface â”‚ Extraction View)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                           â”‚
â”‚  POST /upload â”‚ POST /ask â”‚ POST /extract â”‚ DELETE /documentâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                  â”‚
         â–¼                 â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document      â”‚  â”‚  RAG Engine  â”‚  â”‚   Extractor    â”‚
â”‚   Processor     â”‚  â”‚              â”‚  â”‚                â”‚
â”‚  - PDF Parse    â”‚  â”‚ - Retrieval  â”‚  â”‚ - Structured   â”‚
â”‚  - Chunking     â”‚  â”‚ - LLM Gen    â”‚  â”‚   Extraction   â”‚
â”‚  - Metadata     â”‚  â”‚ - Scoring    â”‚  â”‚ - JSON Schema  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                   â”‚
         â–¼                  â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Vector Store Manager (ChromaDB)                 â”‚
â”‚  - Mistral Embeddings â”‚ Similarity Search â”‚ Persistence     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Upload**: Document â†’ Parse â†’ Chunk â†’ Embed â†’ Store in ChromaDB
2. **Question**: Query â†’ Retrieve top-k chunks â†’ LLM generates answer â†’ Score confidence â†’ Apply guardrails
3. **Extract**: Retrieve document chunks â†’ LLM extracts fields â†’ Validate JSON â†’ Return structured data

## ğŸ§  Technical Approach

### Chunking Strategy

- **Method**: RecursiveCharacterTextSplitter
- **Chunk Size**: 1000 characters
- **Overlap**: 200 characters
- **Separators**: Prioritizes paragraphs (`\n\n`), lines (`\n`), sentences (`. `), then words
- **Metadata**: Each chunk includes document ID, filename, chunk index, and total chunks

**Rationale**: This approach preserves document structure while ensuring semantic completeness. The overlap prevents information loss at chunk boundaries, critical for logistics documents where context matters.

### Retrieval Method

- **Vector Database**: ChromaDB with persistent storage
- **Embeddings**: Mistral Embed (`mistral-embed` model)
- **Retrieval**: Similarity search with top-k=3 chunks
- **Scoring**: L2 distance converted to similarity scores (1 - normalized_distance)

**Rationale**: ChromaDB provides efficient local vector storage. Top-k=3 balances context richness with LLM token limits. Mistral embeddings are cost-effective and performant for domain-specific text.

### Guardrails Approach

1. **Similarity Threshold**: Minimum 0.5 average retrieval score required
2. **Confidence Threshold**: Answers below 0.5 confidence trigger refusal
3. **Grounding Instructions**: LLM explicitly instructed to answer only from context
4. **Source Citation**: All answers include source chunks for verification

**Rationale**: Multi-layer guardrails prevent hallucinations. The 0.5 threshold was chosen to balance recall (answering valid questions) with precision (avoiding low-confidence responses).

### Confidence Scoring Method

**Formula**: `Confidence = (0.7 Ã— Retrieval Similarity) + (0.3 Ã— Source Agreement)`

- **Retrieval Similarity**: Average similarity score from top-k chunks
- **Source Agreement**: 1.0 if multiple sources available, 0.7 otherwise
- **Categories**: High (â‰¥0.7), Medium (0.5-0.7), Low (<0.5)

**Rationale**: Weights retrieval quality heavily (70%) as it's the primary signal, while source agreement (30%) adds confidence when multiple chunks support the answer.

### Retry Mechanism

- **Library**: `tenacity` with exponential backoff
- **Max Retries**: 3 attempts
- **Backoff**: 1s â†’ 2s â†’ 4s â†’ 8s (max 10s)
- **Retry Conditions**: Rate limits (429), token limits, service errors (503)
- **Applied To**: All LLM calls (Groq) and embedding calls (Mistral)

**Rationale**: Protects against transient API failures and rate limits without overwhelming the services.

## ğŸš€ Installation & Setup

### Prerequisites

- Python 3.10+
- API Keys for:
  - Groq (for LLM)
  - Mistral AI (for embeddings)

### Local Setup

1. **Clone and navigate to directory**
```bash
cd "/Users/harshalnelge/Desktop/TMS AI"
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**

The `.env` file already contains:
```env
MISTRAL_API_KEY=your_key_here
GROQ_API_KEY=your_key_here
```

5. **Run the FastAPI server**
```bash
uvicorn app:app --reload --port 8000
```

6. **Run the Streamlit UI** (in a new terminal)
```bash
streamlit run ui/streamlit_app.py --server.port 8501
```

7. **Access the application**
- API Documentation: http://localhost:8000/docs
- Streamlit UI: http://localhost:8501

## ğŸ“– Usage Guide

### Via Streamlit UI

1. **Upload Document** â†’ Navigate to "Upload Document" tab â†’ Choose file â†’ Click "Process Document"
2. **Ask Questions** â†’ Go to "Ask Questions" tab â†’ Enter question â†’ View answer with confidence and sources
3. **Extract Data** â†’ Go to "Extract Data" tab â†’ Click "Extract Data" â†’ Download JSON

### Via API

**Upload Document:**
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@path/to/document.pdf"
```

**Ask Question:**
```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "your-document-id",
    "question": "What is the carrier rate?"
  }'
```

**Extract Structured Data:**
```bash
curl -X POST "http://localhost:8000/extract?document_id=your-document-id"
```

## ğŸ“‹ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check |
| `/upload` | POST | Upload and process document |
| `/ask` | POST | Ask question about document |
| `/extract` | POST | Extract structured shipment data |
| `/document/{id}` | DELETE | Delete document and vector store |

Full API documentation available at http://localhost:8000/docs when server is running.

## âš ï¸ Known Failure Cases

1. **Scanned PDFs**: Text extraction fails on image-based PDFs (requires OCR)
2. **Complex Tables**: Table structure may be lost during chunking
3. **Multi-Document References**: Cannot answer questions spanning multiple uploaded documents
4. **Ambiguous Questions**: Vague questions may trigger guardrails even with relevant content
5. **Large Documents**: Very large files (>10MB) are rejected due to size limits
6. **Missing Fields**: Extraction returns null for fields not present in document

## ğŸ’¡ Future Improvements

1. **OCR Integration**: Add Tesseract/Azure Form Recognizer for scanned documents
2. **Table Extraction**: Specialized chunking for tabular data
3. **Multi-Document RAG**: Query across multiple uploaded documents
4. **Streaming Responses**: Real-time answer generation for better UX
5. **Fine-tuned Embeddings**: Domain-specific embedding model for logistics
6. **Citation Precision**: Line-level source highlighting in UI
7. **Batch Processing**: Upload and process multiple documents simultaneously
8. **Persistent Storage**: Database backend instead of in-memory registry
9. **User Authentication**: Multi-user support with document isolation
10. **Advanced Analytics**: Track question patterns and confidence trends

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI 0.109.0
- **UI**: Streamlit 1.31.0
- **LLM**: ChatGroq (llama-3.1-8b-instant)
- **Embeddings**: Mistral Embed
- **Vector DB**: ChromaDB 0.4.22
- **Document Processing**: LangChain, PyPDF, python-docx
- **Retry Logic**: Tenacity 8.2.3

## ğŸ“ Project Structure

```
TMS AI/
â”œâ”€â”€ app.py                      # FastAPI application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ .gitignore                 # Git ignore patterns
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py            # Configuration management
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schemas.py             # Pydantic data models
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ document_processor.py # Document parsing and chunking
â”‚   â”œâ”€â”€ vector_store.py        # ChromaDB vector management
â”‚   â”œâ”€â”€ rag_engine.py          # RAG Q&A with guardrails
â”‚   â””â”€â”€ extractor.py           # Structured data extraction
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ helpers.py             # Utility functions
â”‚   â””â”€â”€ retry_utils.py         # Retry mechanisms
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py       # Streamlit user interface
â”œâ”€â”€ uploads/                   # Uploaded documents (gitignored)
â”œâ”€â”€ chroma_db/                 # Vector database (gitignored)
â””â”€â”€ docs/
    â””â”€â”€ ARCHITECTURE.md        # Detailed architecture docs
```

## ğŸ“ License

This is a POC project created for demonstration purposes.

## ğŸ‘¤ Author

Built with â¤ï¸ for TMS AI Assignment

---

**Questions or Issues?** Check the API docs at `/docs` or review the logs for debugging.
